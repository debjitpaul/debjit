<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://debjitpaul.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debjitpaul.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-17T15:57:00+00:00</updated><id>https://debjitpaul.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">REFINER:Reasoning Feedback on Intermediate Representations</title><link href="https://debjitpaul.github.io/blog/2023/refiner/" rel="alternate" type="text/html" title="REFINER:Reasoning Feedback on Intermediate Representations"/><published>2023-04-04T19:53:00+00:00</published><updated>2023-04-04T19:53:00+00:00</updated><id>https://debjitpaul.github.io/blog/2023/refiner</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2023/refiner/"><![CDATA[<p>Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT-3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be substituted with humans at inference time.</p>]]></content><author><name></name></author><category term="pre-print"/><category term="blockquotes"/><summary type="html"><![CDATA[Reasoning Feedback on Intermediate Representations]]></summary></entry><entry><title type="html">Social Commonsense Reasoning with Structured Knowledge in Text</title><link href="https://debjitpaul.github.io/blog/2022/phd-thesis/" rel="alternate" type="text/html" title="Social Commonsense Reasoning with Structured Knowledge in Text"/><published>2022-04-23T19:53:00+00:00</published><updated>2022-04-23T19:53:00+00:00</updated><id>https://debjitpaul.github.io/blog/2022/phd-thesis</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2022/phd-thesis/"><![CDATA[<p>Understanding a social situation requires the ability to reason about the underlying emotions and behaviour of others. For example, when we read a \textit{personal story}, we use our prior commonsense knowledge and social intelligence to infer the emotions, motives, and anticipate the actions of the characters in a story. For machines to understand text related to <em>personal stories and social conversations</em>, they must be able to make commonsense inferences. While most people can reason deeply about the social implications of the text, it is challenging for natural language processing systems as these implications are often subtle and implicit.</p> <p>This dissertation argues that NLP systems must learn to reason more explicitly about the underlying social knowledge in text to perform social commonsense reasoning. We divide the above argument into two sub-problems: (i) understanding the underlying social knowledge and (ii) explicitly reasoning about such knowledge for social commonsense reasoning. To address these problems, we propose building NLP systems that integrate neural network based learning with structured knowledge representations.</p> <p>In the first part of this dissertation, we study the role of structured commonsense knowledge in understanding the social dynamics of characters and their actions in stories. Our motivation behind enriching the model with structured commonsense knowledge is to bridge the gap between surface meanings of texts and the underlying social implication of each event in the stories. We develop a novel model that incorporates commonsense knowledge into neural models and showcases the importance of commonsense knowledge in understanding social dynamics of story characters. Further, we investigate the role of temporal dynamics of story events in understanding social situations. We develop a model that can explicitly learn about <em>what social event follows another event</em> from personal narrative stories. We demonstrate that <em>implicitly</em> leveraging such temporal knowledge about story events can support social commonsense reasoning tasks.</p> <p>In the second part of this dissertation, we investigate methods to explicitly reason about the knowledge related to social dynamics of characters (<em>behavior, mental states</em>) and cause/effect of social events. We propose a novel model named as <em>multi-head knowledge attention</em> that incorporates such social knowledge into state-of-the-art neural NLP models to address two complex commonsense inference tasks. We demonstrate that our method of incorporating knowledge can improve â€“ (i) the robustness and the interpretability of the model and (ii) the overall performance of the model compared to other knowledge integration methods. We also aim to investigate social commonsense reasoning as a natural language generation task. We design a story completion task that requires natural language generation models to perform both forward and backward reasoning. We study the role of contextualized commonsense knowledge in natural language generation tasks. We propose a model that jointly learns to generate contextualized inference rules as well as narrative stories. We demonstrate that our model can outperform state-of-the-art non-contextualized commonsense knowledge-based generation models.</p> <p>We hope that the research presented in this dissertation will open up interesting scopes for future research involving social commonsense reasoning and other related topics.</p>]]></content><author><name></name></author><category term="PhD_Thesis"/><category term="blockquotes"/><summary type="html"><![CDATA[Social Commonsense Reasoning with Structured Knowledge in Text]]></summary></entry><entry><title type="html">Graph-based Multi-Hop Commonsense Knowledge</title><link href="https://debjitpaul.github.io/blog/2019/eurnlp/" rel="alternate" type="text/html" title="Graph-based Multi-Hop Commonsense Knowledge"/><published>2019-10-11T19:53:00+00:00</published><updated>2019-10-11T19:53:00+00:00</updated><id>https://debjitpaul.github.io/blog/2019/eurnlp</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2019/eurnlp/"><![CDATA[<p>Understanding narrative and argumentative text often requires knowledge beyond the text. While reading such texts, as humans, we are competent in distilling and performing inference by applying commonsense knowledge. Although there has been significant progress in neural machine reading and understanding (using powerful language models), there is still a performance gap between machines and humans, especially when it requires implicit knowledge. One of the reasons being that commonsense knowledge is not often explicitly stated in natural language text. We aim to solve this issue by leveraging knowledge from resources such as ConceptNet. However, identifying contextually relevant information from such a large knowledge base is a non-trivial task. In this work, we propose an effective two-step method to extract relevant multi-hop knowledge paths from the chosen knowledge resource that associate concepts in a given textual sequence: (i) collect all potentially relevant knowledge relations among concepts that appear in the text in a subgraph; (ii) rank, filter and select high-quality multi-hop paths using graph-based local measures and graph centrality algorithms. Further, we propose a neural model that uses a gated attention mechanism to incorporate relevant multi-hop commonsense knowledge paths. We evaluate our model on two different tasks: (i) Argumentation relation classification (the task of determining <em>support or attack</em> relations that hold between two arguments), (ii) determining the human needs (multi-label classification task) of story characters given a narrative story. We show considerable improvement above strong knowledge-agnostic baselines. Our model offers interpretability through the learned attention map over commonsense knowledge paths.</p> <p><a href="">Read More</a></p>]]></content><author><name></name></author><category term="EurNLP"/><category term="blockquotes"/><summary type="html"><![CDATA[Graph-based Multi-Hop Commonsense Knowledge]]></summary></entry></feed>