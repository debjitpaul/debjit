---
layout: post
title: Graph-based Multi-Hop Commonsense Knowledge
date: 2019-10-11 15:53:00-0400
description: Graph-based Multi-Hop Commonsense Knowledge
categories: EurNLP blockquotes
giscus_comments: true
related_posts: true
---



Understanding narrative and argumentative text often requires knowledge beyond the text. 
While reading such texts, as humans, we are competent in distilling and performing inference by applying commonsense knowledge. 
Although there has been significant progress in neural machine reading and understanding (using powerful language models), there is still a performance gap between machines and humans, especially when it requires implicit knowledge.
One of the reasons being that commonsense knowledge is not often explicitly stated in natural language text. 
We aim to solve this issue by leveraging knowledge from resources such as ConceptNet. 
However, identifying contextually relevant information from such a large knowledge base is a non-trivial task. 
In this work, we propose an effective two-step method to extract relevant multi-hop knowledge paths from the chosen knowledge resource that associate concepts 
in a given textual sequence: (i) collect all potentially relevant knowledge relations  among concepts that appear in the text in a subgraph; (ii) rank, filter and select high-quality multi-hop paths using graph-based local measures and graph centrality algorithms. 
Further, we propose a neural model that uses a gated attention mechanism to incorporate relevant multi-hop commonsense knowledge paths. We evaluate our model on two different tasks:  (i) Argumentation relation classification (the task of determining \textit{support or attack} relations that hold between two arguments) \cite{stab-gurevych-2014-annotating}, 
(ii) determining the human needs (multi-label classification task) of story characters given a narrative story \cite{rashkin2018modeling}. 
%We assess the model's performance on two datasets: (i) Student Essay dataset v.02, which covers 402 essays for the argumentation classification task (binary classification), (ii) Modeling Naive Psychology of Characters in Simple Commonsense Stories (MNPCSCS) dataset \cite{rashkin2018modeling} for human need (19 fine-grained class) classification. 
We show considerable improvement above strong knowledge-agnostic baselines (Table \ref{tab:all}, \ref{tab:rank}). 
Our model offers interpretability through the learned attention map over commonsense knowledge paths (Fig.\ \ref{fig:attention}).
